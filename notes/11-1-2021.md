# Neural Rendering Updates (November 1, 2021)
## NeRFMM Execution On Holography Image
"Neural Radiance Fields Without Known Camera Parameters" [[arxiv]](https://arxiv.org/pdf/2102.07064.pdf)

NeRF is trained by minimising the photometric reconstruction error on the input views.
Camera parameter is jointly optimized during the NeRF process.
```
Estimate camera parameters of: focal length, rotation and translation matrix.
Training start with N images:
    M <- randomly selected pixel locations, which we would like to reconstruct from the NeRF model
    for M pixel locations:
        Construct a ray from the camera position through the pixel into the radiance field
        for within the ray range:
            Jointly optimise the camera parameters and scene representation
            Apply forward NeRF
        Rendering function (obtain the colour of the pixel by aggregating the predicted radiance and densities along the ray)
    For each reconstructed pixel, compute photometric loss by comparing its predicted colour
    Backward loss
    Update the optimizer
```

- GitHub: https://github.com/ActiveVisionLab/nerfmm
- Execution Flow:
  1. Holography Image Preprocessing
  2. Optimize Camera Parameters + NeRF Optimization
        ```
        python tasks/any_folder/train.py --base_dir='data' --scene_name='train_scene' --gpu_id='2'
        ```
  3. View Rendering
        ```
        python tasks/any_folder/spiral.py --base_dir='data' --scene_name='train_scene' --ckpt_dir='logs/any_folder/train_scene/<ckpt-folder>' --gpu_id='2'
        ```
<hr/>

## NeRFMM Output
1. Raw images (Box folder: `LightField_DSLR_16by11/RAW-JPEG`)
2. Calibrated & Registered images (Box folder: `LightField_DSLR_16by11/Registered_Cropped`)
3. Cropped images (cropped images only from #2)
   
|                 Marker Detection                  |                    Corners Calculated                    |                Cropped Image Rotation                 |
| :-----------------------------------------------: | :------------------------------------------------------: | :---------------------------------------------------: |
| ![](../nerfmm_output/train_raw/video_out/img.gif) | ![](../nerfmm_output/train_calibrated/video_out/img.gif) | ![](../nerfmm_output/train_cropped/video_out/img.gif) |

- Image crop script with examples: [`registered_preprocessing.ipynb`](../image_preprocessing/registered_preprocessing.ipynb)
- Holography imgae crop Script: [`registered_preprocessing.py`](../image_preprocessing/registered_preprocessing.py)
- Preprocessing Execution:
    ```
    python registered_preprocessing.py <imread_directory> <imsave_directory>
    ```
- Camera calibrations and registrations were already done ([reference](./../image_preprocessing/Register.m)), so hardcoded the 4 corner locations.
<hr/>

## Quantitative Measures Review
1. Set the ground truth and the test dataset. Randomly split the train/test set into 80-20% ratio, and then try to render the test set with the trained network (`ckpt` weight-saved model).
2. Compare the trained ground truth rendering output vs. test set rendering output, by applying the following:
   - PSNR ([OpenCV](https://dsp.stackexchange.com/a/61510))
   - SSIM ([Python Example](https://ourcodeworld.com/articles/read/991/how-to-calculate-the-structural-similarity-index-ssim-between-two-images-with-python))
   - LPIPS ([GitHub](https://github.com/richzhang/PerceptualSimilarity))
