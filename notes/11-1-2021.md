# Neural Rendering Updates (November 1, 2021)
## Problem with NeRF
NeRF needs camera position obtained by COLMAP.
COLMAP needs camera position (3 by 4 matrix) and instrinsic parameters including `hwf = [height, width, focal length]`. In addition, fov with view frustum values are needed. In order to get the camera rotation and translation matrix, intrinsic parameters are required, but the original dataset did not have a measurement of intrinsic parameters.

<hr/>

## NeRFMM Execution On Holography Image
"Neural Radiance Fields Without Known Camera Parameters" [[arxiv]](https://arxiv.org/pdf/2102.07064.pdf)

NeRF is trained by minimising the photometric reconstruction error on the input views.
Camera parameter is jointly optimized during the NeRF process.

Algorithms:
```
Estimate camera parameters of: focal length, rotation and translation matrix.
Training start with N images:
    M <- randomly selected pixel locations, which we would like to reconstruct from the NeRF model
    for M pixel locations:
        Construct a ray from the camera position through the pixel into the radiance field
        for within the ray range:
            Jointly optimise the camera parameters and scene representation
            Apply forward NeRF
        Rendering function (obtain the colour of the pixel by aggregating the predicted radiance and densities along the ray)
    For each reconstructed pixel, compute photometric loss by comparing its predicted colour
    Backward loss
    Update the optimizer
```

- GitHub: https://github.com/ActiveVisionLab/nerfmm
- Execution Flow:
  1. Holography Image Preprocessing
  2. Optimize Camera Parameters + NeRF Optimization
        ```
        python tasks/any_folder/train.py --base_dir='data' --scene_name='train_scene' --gpu_id='2'
        ```
  3. View Rendering
        ```
        python tasks/any_folder/spiral.py --base_dir='data' --scene_name='train_scene' --ckpt_dir='logs/any_folder/train_scene/<ckpt-folder>' --gpu_id='2'
        ```
<hr/>

## NeRFMM Output
1. Raw images (Box folder: `LightField_DSLR_16by11/RAW-JPEG`)
2. Calibrated & Registered images (Box folder: `LightField_DSLR_16by11/Registered_Cropped`)
3. Cropped images (cropped images only from #2)
   
|             Raw Train Reconstruction              |       Calibrated & Registered Train Reconstruction       |             Cropped Train Reconstruction              |
| :-----------------------------------------------: | :------------------------------------------------------: | :---------------------------------------------------: |
| ![](../nerfmm_output/train_raw/video_out/img.gif) | ![](../nerfmm_output/train_calibrated/video_out/img.gif) | ![](../nerfmm_output/train_cropped/video_out/img.gif) |

<hr/>

## NeRFMM Mixed Train/Test Set
1. Train-raw & Test-cropped
2. Train-cropped & Test-raw

|             Train-raw & Test-cropped              |             Train-cropped & Test-raw              |
| :-----------------------------------------------: | :-----------------------------------------------: |
| ![](../nerfmm_output/train_mixed/raw_fit_img.gif) | ![](../nerfmm_output/train_mixed/fit_raw_img.gif) |

## Quantitative Measures Review
1. Set the ground truth and the test dataset. Split the train/test set into 80-20% ratio based on their viewpoints, and then try to render the test set with the trained network (`ckpt` weight-saved model).
2. Compare the trained ground truth rendering output vs. test set rendering output, by applying the following:
   - PSNR ([OpenCV](https://dsp.stackexchange.com/a/61510))
   - SSIM ([Python Example](https://ourcodeworld.com/articles/read/991/how-to-calculate-the-structural-similarity-index-ssim-between-two-images-with-python))
   - LPIPS ([GitHub](https://github.com/richzhang/PerceptualSimilarity))

<hr/>

## Dataset Obtained
- Images were taken using a DSLR with a 50 mm lens
- Calibrated to remove lens distortion later on
- The camera moved in an x-y plane, with a vertical step size of 3.7 cm and a horizontal step size of 2.3 cm; the distance between the hologram plane and the lens was approx. 1.11 m
- The diameter of small inner circle in each fiducial marker is approx 0.53cm
- The width of black hologram panel holder is 3in
<hr/>

## Holography Image Preprocessing for Registered & Calibrated Images
- Image crop script with examples: [`registered_preprocessing.ipynb`](../image_preprocessing/registered_preprocessing.ipynb)
- Holography imgae crop Script: [`registered_preprocessing.py`](../image_preprocessing/registered_preprocessing.py)
- Preprocessing Execution:
    ```
    python registered_preprocessing.py <imread_directory> <imsave_directory>
    ```
- Camera calibrations and registrations were already done ([reference](./../image_preprocessing/Register.m)), so hardcoded the 4 corner locations.