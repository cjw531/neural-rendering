# Neural Rendering Updates (October 18, 2021)
## NeRF Execution
- GitHub: https://github.com/bmild/nerf
- Execution Flow:
  1. Input Images (no preprocessing needed)
  2. NeRF Optimization (training)
  3. Images COLMAP
  4. NeRF Rendering
- Model checkpoint information
  - Save as `*.npy` format, in every 10,000 iteration.
  - Saved in: `~/nerf/logs/SCENE/*`

### 1. Fern Dataset
- Trained with 1,000,000 iterations, and validate its overfitting result with the Tensorboard output:
![](../nerf_output/fern/fern_loss.png "Fern Loss Plot")
- This loss plot aligns with the author's argument that training with 200K iterations is recommended.
- Rendering with `render_demo.ipynb` [[Video Output]](https://user-images.githubusercontent.com/25876799/137808785-4d90bd21-3ca6-406a-b89f-77adc5733f71.mp4) [[Raw Output]](https://github.com/cjw531/neural-rendering/blob/main/nerf_output/fern/video.mp4)
- Rendering with `extract_mesh.ipynb` is does not work on fern dataset:
![](../nerf_output/fern/fern_mesh.png "Fern Mesh")

### 2. Lego Dataset
- Synthetic dataset is already splitted into train/validation/test set.
- Running a COLMAP on the synthetic dataset and obtaining a camera position throws an error.
- `poses_bounds.npy` is necessary in order to render it.
- Rendering with `extract_mesh.ipynb` is possible, and results depth map ans well as the mesh result [[Video Output]](https://user-images.githubusercontent.com/25876799/137808267-b5278b4f-2974-4fde-8268-c1b055ad333c.mp4)[[Raw Output]](https://github.com/cjw531/neural-rendering/blob/main/nerf_output/lego/lego_mesh_turntable.mp4):
![](../nerf_output/lego/lego_mesh.png "Lego Mesh")
![](../nerf_output/lego/lego_rgb.png "Lego RGB")

### 3. Gerrard Hall Dataset
- This is the image dataset that was missing the top part of the building.
- This image cannot be rendered through NeRF because NeRF requires pre-COLMAP processing and need to obtain the camera position which is `poses_bounds.npy`.
- COLMAP output shows that it failed to extract the feature, and running LLFF's `imgs2poses.py` throws an error as below.

```
Traceback (most recent call last):
  File "imgs2poses.py", line 18, in <module>
    gen_poses(args.scenedir, args.match_type)
  File "/host/home/jiwon/workspace/LLFF/llff/poses/pose_utils.py", line 274, in gen_poses
Need to run COLMAP
Features extracted
Features matched
Sparse map created
Finished running COLMAP, see ../img_dataset/gerrard-hall/colmap_output.txt for logs
Post-colmap
    poses, pts3d, perm = load_colmap_data(basedir)
  File "/host/home/jiwon/workspace/LLFF/llff/poses/pose_utils.py", line 14, in load_colmap_data
    camdata = read_model.read_cameras_binary(camerasfile)
  File "/host/home/jiwon/workspace/LLFF/llff/poses/colmap_read_model.py", line 115, in read_cameras_binary
    with open(path_to_model_file, "rb") as fid:
IOError: [Errno 2] No such file or directory: '../img_dataset/gerrard-hall/sparse/0/cameras.bin'
```

### 4. Pinecone Dataset
- Successfully obtained `poses_bounds.npy` via COLMAP and LLFF.
- Optimization output as follows (50,000 iterations so far):
  1. Disparity Map [[Video Output]](https://user-images.githubusercontent.com/25876799/137809483-a2c4dd27-cd14-4f50-864e-bf2bc42c457f.mp4) [[Raw Video]](https://github.com/cjw531/neural-rendering/blob/main/nerf_output/pinecone/pinecone_test_spiral_050000_disp.mp4)
  2. RGB Map [[Video Output]](https://user-images.githubusercontent.com/25876799/137809619-f8e2a82d-de29-4434-912c-34c4b6256c80.mp4) [[Raw Video]](https://github.com/cjw531/neural-rendering/blob/main/nerf_output/pinecone/pinecone_test_spiral_050000_rgb.mp4)
  3. RGB Still [[Video Output]](https://user-images.githubusercontent.com/25876799/137809533-ca905250-c3b6-450d-ace5-59e93cdaec38.mp4) [[Raw Video]](https://github.com/cjw531/neural-rendering/blob/main/nerf_output/pinecone/pinecone_test_spiral_050000_rgb_still.mp4)

<hr/>

## LLFF Camera Pose
### 0. Installation
- Before following [these steps](https://github.com/Fyusion/LLFF#installation-tldr-setup-and-render-a-demo-scene) in LLFF, [docker](https://docs.docker.com/engine/install/ubuntu/#install-using-the-repository) and [nvidia-docker](https://medium.com/@linhlinhle997/how-to-install-docker-and-nvidia-docker-2-0-on-ubuntu-18-04-da3eac6ec494) installation were needed.

### 1. Execution
- Run the following single-command in LLFF directory, to obtain `poses_bounds.npy`:
```
$ sudo nvidia-docker run --rm --volume /:/host --workdir /host$PWD tf_colmap python imgs2poses.py <IMG_DATASET_PATH>
```
- Running the auto-command does not work, throws segmentation fault (unnecessary anyways):
```
$ sudo nvidia-docker run -it --rm --volume /:/host --workdir /host$PWD tf_colmap
```
```
demo.sh: line 24: 106 Segmentation fault (core dumped) cuda_renderer/cuda_renderer data/testscene/mpis_360 data/testscene/outputs/test_path.txt data/testscene/outputs/test_vid.mp4 360 .8 18
```
- Minimum GPU RAM is 4GB, while the local has 2GB memory, and throws OOM error if multiple commands are running together [(Reference)](https://github.com/Fyusion/LLFF/issues/11#issuecomment-515771554).
- Tried [edits suggested](https://github.com/Fyusion/LLFF/issues/11#issuecomment-516905659) under the GitHub issue, which is reducing the network patch, but did not work.

<hr/>

## COLMAP Rendering
- COLMAP Rendering can be obtained with the [following CLI](https://colmap.github.io/cli.html).
- For the advanced rendering (other than `mapper`, but `*.ply` models), [CUDA toolkit](https://linuxconfig.org/how-to-install-cuda-on-ubuntu-20-04-focal-fossa-linux) is needed (7.X higher).
- Although CUDA has installed in local, colmap throws an error saying that CUDA is not available in this machine.
### 1. Pinecone
- Pinecone `mapper` output:
![](../colmap_output/pinecone/pinecone_colamp.png "Fern Loss Plot")
